{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # -- IMPORTS AND CONFIGS --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, TimeSeriesSplit\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "from sklearn import set_config\n",
    "from feature_engine import discretisation, encoding\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, FunctionTransformer, OrdinalEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import TargetEncoder\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "import sys \n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))\n",
    "from src.eng_funcs import EngenhariaDatas, ModelAuditor, FatorCalibracao, remover_periodo_outlier, criar_features_temporais\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# FORCE ALL THE SKLEARN TRANSFORMERS RETURNING PANDAS DATAFRAME\n",
    "set_config(transform_output=\"pandas\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## --- VARIABLES CONFIG ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shifts_periods =[1, 12]\n",
    "rollings_periods = [3, 6, 12]\n",
    "meses_hist_oot = 3                   # ---> USING LAST 3 MONTHS\n",
    "fator_calibracao_final = 0.79        # USED 0.6 VALUE BECAUSE AFTER MODEL ANALYSIS WE CAN SEE MODEL TRENDS OVERESTIMATE PREDICT VALUES (IN COMPARING W/ OOT DATASET) AND BECAUSE OF THAT WAS IDENTIFIED A MINUMUM COMMOM VALUE THAT CALIBRATE ALL THE PREDICT RESULTS\n",
    "groupcol = 'emitente_categoria' \n",
    "target_col = 'Soma_Total_Maximos'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # -- READ AND SAMPLE DATASET --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_first = pd.read_parquet('FinalTable_Consolidada.parquet', engine='fastparquet')\n",
    "df_first.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## --- DENSIFIING DATASET W/ NEW FEATURES ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agrupado = df_first.groupby([groupcol, 'periodo'])['Soma_Total_Maximos'].sum().reset_index().copy()\n",
    "\n",
    "duplicatas = df_agrupado.duplicated(subset=[groupcol, 'periodo']).sum()\n",
    "if duplicatas > 0:\n",
    "    raise ValueError(f\"Ainda existem {duplicatas} duplicatas! Verifique seus dados.\")\n",
    "\n",
    "# REINDEX PERIODO+PESSOAS\n",
    "min_date = df_agrupado['periodo'].min()\n",
    "max_date = df_agrupado['periodo'].max()\n",
    "\n",
    "all_dates = pd.date_range(start=min_date, end=max_date, freq='MS')\n",
    "all_customers_cat_emit = df_agrupado[groupcol].unique()\n",
    "\n",
    "index_full = pd.MultiIndex.from_product(\n",
    "    [all_customers_cat_emit, all_dates],\n",
    "    names=[groupcol,'periodo']\n",
    ")\n",
    "\n",
    "df_agrupado_full = df_agrupado.set_index([groupcol,'periodo']).reindex(index_full)\n",
    "df_agrupado_full['Soma_Total_Maximos'] = df_agrupado_full['Soma_Total_Maximos'].fillna(0)\n",
    "\n",
    "df_agrupado_full = df_agrupado_full.reset_index().copy()\n",
    "\n",
    "# FEATURE ENGENEERING | FUNCTION CREATE criar_features_temporais\n",
    "df_agrupado_full = criar_features_temporais(df_agrupado_full,\n",
    "                                            col_target='Soma_Total_Maximos',\n",
    "                                            groupcols=['emitente_categoria'],\n",
    "                                            shifts= shifts_periods,\n",
    "                                            rollings = rollings_periods)\n",
    "df_agrupado_full = df_agrupado_full.drop(columns=['Soma_Total_Maximos'])\n",
    "\n",
    "# --- JOINING MAIN TABLE W/ NEW TABLE CONTAINS NEW FEATURES --- \n",
    "df = pd.merge(df_first, \n",
    "              df_agrupado_full,\n",
    "              on=['emitente_categoria', 'periodo'],\n",
    "              how='left').copy()\n",
    "\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # -- MINI EDA - EXPLORATORY DATA ANALYSIS --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.isnull().sum().sort_values(ascending=False))\n",
    "\n",
    "# DISCOVERING NULL VALUES PERIOD\n",
    "df.sort_values(by=['emitente_categoria','periodo']).groupby('periodo')['valor_total_emitente_categoria_shift_12'].apply(lambda x: x.isna().sum())\n",
    "\n",
    "# DROPPING NULL PERIODS FROM DATASET\n",
    "df = remover_periodo_outlier(\n",
    "    df, \n",
    "    col_data='periodo', \n",
    "    inicio='2020-01-01', \n",
    "    fim='2020-12-31'\n",
    ")\n",
    "df = df.reset_index().copy()\n",
    "\n",
    "# /*****************************************************************\\\n",
    "# DEFAULT CORRELATION\n",
    "# COLS TO DATASET DROP\n",
    "cols_to_drop = ['documento_pessoa', 'cod_agencia', 'cod_cooperativa', 'cod_central', 'Contagem_contratos'] # Exemplo\n",
    "df_corr = df.drop(columns=cols_to_drop, errors='ignore')\n",
    "\n",
    "# GENERAL CORRELATION ARRAY\n",
    "correlacao = df_corr.corr(numeric_only=True, method='pearson')\n",
    "mask = np.triu(np.ones_like(correlacao, dtype=bool)) ----> DROPPING GRAPH MIRROR\n",
    "plt.figure(figsize=(35,28),dpi = 300 )\n",
    "sns.heatmap(correlacao, \n",
    "            mask=mask,               # APPLYING MASK\n",
    "            annot=True,              # SHOWYING NUMBERS\n",
    "            fmt=\".2f\",               # FORMATING (2 POINTS AFTER DOT)\n",
    "            cmap='coolwarm',         # RED-BLUE (REVERSE): RED=HIGH, BLUE=LOW \"RdBu_r\"\n",
    "            vmax=1,                  # KEEP SCALE (MANDATORY em ML)\n",
    "            vmin=-1,                 # KEEP SCALE (MANDATORY em ML)\n",
    "            center=0,                # KEEP SCALE (MANDATORY em ML)\n",
    "            square=True,             # ENSURE PERFECT SQUARES\n",
    "            linewidths=.5,           # WHITE LINES FOR SEPARATE DATA\n",
    "            cbar_kws={\"shrink\": .5}  # INCREASE BAR SIDE TO KEEP ELEGANT\n",
    "            )\n",
    "plt.title('CorrelaÃ§Ã£o Variveis')\n",
    "plt.show()\n",
    "\n",
    "# /*****************************************************************\\\n",
    "# TARGET CORRELATION\n",
    "# CALCULATE CORRELATION AROUND ALL THE VARIABLES AGAINST THE TARGET | KEEPING MOST STRONG CORRELATED VARIABLES ON TOP\n",
    "corr_target = df_corr.corr(numeric_only=True)[[target_col]].sort_values(by=target_col, ascending=False)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(4,10), dpi = 300)\n",
    "sns.heatmap(corr_target, \n",
    "            annot=True, \n",
    "            cmap='coolwarm', \n",
    "            vmin=-1, vmax=1, \n",
    "            cbar=False,\n",
    "            fmt='.2f'\n",
    "           )\n",
    "\n",
    "plt.title(f'CorrelaÃ§Ã£o com {target_col}', fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "# /*****************************************************************\\\n",
    "# TOP 10 VARIABLES CORRELATION\n",
    "cols_analise = ['Soma_Total_Maximos'] + [c for c in df.columns if 'valor_total_' in c]\n",
    "\n",
    "corr = df[cols_analise].corr()\n",
    "\n",
    "print(\"--- CorrelaÃ§Ã£o com o Alvo (Top 10) ---\")\n",
    "print(corr['Soma_Total_Maximos'].sort_values(ascending=False).head(10))\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(30,25))\n",
    "sns.heatmap(corr, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.show()\n",
    "    \n",
    "# /*****************************************************************\\\n",
    "# SEASONALITY FOR EMITENTE CATEGORIA\n",
    "top_cats = df.groupby('emitente_categoria')['Soma_Total_Maximos'].sum().nlargest(3).index\n",
    "df_plot = df[df['emitente_categoria'].isin(top_cats)]\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "sns.lineplot(data=df_plot, x='periodo', y='Soma_Total_Maximos', hue='emitente_categoria', marker='o')\n",
    "plt.title(\"Comportamento Temporal das Top 3 Categorias\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # -- X/Y - TRAIN TESTE AND OOT - OUT OF TIME --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# OOT - OUT OF TIME\n",
    "data_corte = df['periodo'].max() - pd.DateOffset(months = meses_hist_oot) # ---> SEPARATING DATA BY DATE\n",
    "oot = df[df['periodo'] >= data_corte].copy()\n",
    "oot = oot.sort_values(by=['emitente_categoria','periodo'], ascending=True)\n",
    "\n",
    "# X/Y\n",
    "df_new = df[df['periodo']<data_corte].sort_values(by=['emitente_categoria','periodo'], ascending=True).copy()\n",
    "target = 'Soma_Total_Maximos'\n",
    "\n",
    "X, y = df_new.drop(columns=[target], errors='ignore'), df_new[target]\n",
    "\n",
    "# X/Y - TRAIN TESTE\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    random_state=None,\n",
    "                                                    shuffle=False,\n",
    "                                                    test_size=0.2)\n",
    "\n",
    "\n",
    "# RATE PROPORTION Y VARIABLES\n",
    "print('Taxa da variavel resposta y:', y.mean())\n",
    "print('Taxa da variavel resposta Treino:', y_train.mean())\n",
    "print('Taxa da variavel resposta Teste:', y_test.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # -- SEPARATING VARIABLES IN NOT USED - NUMBER - STRING - CODE - DATE --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 1. NOT USED COLUNS \"EXCLUDED\" (LEAKAGE, ORIGINAL DATE, USELESS IDs)\n",
    "blacklist = ['ticket_medio', \n",
    "             'periodo',\n",
    "             'documento_pessoa',   # <--- DROPPING (OVERFITTING)\n",
    "             'valor_juros',        # <--- DROPPING (DATA LEAKAGE)\n",
    "             'valor_aliq_proagro', # <--- DROPPING (DATA LEAKAGE)\n",
    "             'Contagem_contratos', # <--- DROPPING (ONLY 1 VALUES)\n",
    "             'index',              # <--- DROPPING (COL CREATED IN COLUMN TRANSFORM)\n",
    "             'cod_central',        # <--- DROPPING (COL USED IN OTHER TRANSFORM PIPELINE TYPE)\n",
    "             'cod_agencia',        # <--- DROPPING (COL USED IN OTHER TRANSFORM PIPELINE TYPE)\n",
    "             'cod_cooperativa'     # <--- DROPPING (COL USED IN OTHER TRANSFORM PIPELINE TYPE)\n",
    "            ] \n",
    " \n",
    "# 2. NUMBER COLUMN LIST (INT, FLOAT, FLAG)\n",
    "num_vars = ['periodo_sin', 'periodo_cos', \n",
    "            'valor_total_emitente_categoria_shift_1', \n",
    "            'valor_total_emitente_categoria_shift_1_rolling_3', \n",
    "            'volatilidade_emitente_categoria_shift_1_rolling_3', \n",
    "            'ratio_emitente_categoria_shift_1_rolling_3', \n",
    "            'coef_emitente_categoria_shift_1_rolling_3', \n",
    "            'frequencia_emitente_categoria_shift_1_rolling_3', \n",
    "            'flag_emitente_categoria_shift_1_rolling_3', \n",
    "            'valor_total_emitente_categoria_shift_1_rolling_6', \n",
    "            'volatilidade_emitente_categoria_shift_1_rolling_6', \n",
    "            'ratio_emitente_categoria_shift_1_rolling_6', 'coef_emitente_categoria_shift_1_rolling_6', \n",
    "            'frequencia_emitente_categoria_shift_1_rolling_6', 'flag_emitente_categoria_shift_1_rolling_6', \n",
    "            'valor_total_emitente_categoria_shift_1_rolling_12', \n",
    "            'volatilidade_emitente_categoria_shift_1_rolling_12', \n",
    "            'ratio_emitente_categoria_shift_1_rolling_12', 'coef_emitente_categoria_shift_1_rolling_12', \n",
    "            'frequencia_emitente_categoria_shift_1_rolling_12', \n",
    "            'flag_emitente_categoria_shift_1_rolling_12', 'valor_total_emitente_categoria_shift_12', \n",
    "            'valor_total_emitente_categoria_shift_12_rolling_3', \n",
    "            'volatilidade_emitente_categoria_shift_12_rolling_3', \n",
    "            'ratio_emitente_categoria_shift_12_rolling_3', 'coef_emitente_categoria_shift_12_rolling_3', \n",
    "            'frequencia_emitente_categoria_shift_12_rolling_3', \n",
    "            'flag_emitente_categoria_shift_12_rolling_3', 'valor_total_emitente_categoria_shift_12_rolling_6', \n",
    "            'volatilidade_emitente_categoria_shift_12_rolling_6', \n",
    "            'ratio_emitente_categoria_shift_12_rolling_6', 'coef_emitente_categoria_shift_12_rolling_6', \n",
    "            'frequencia_emitente_categoria_shift_12_rolling_6', 'flag_emitente_categoria_shift_12_rolling_6', \n",
    "            'valor_total_emitente_categoria_shift_12_rolling_12', \n",
    "            'volatilidade_emitente_categoria_shift_12_rolling_12', \n",
    "            'ratio_emitente_categoria_shift_12_rolling_12', 'coef_emitente_categoria_shift_12_rolling_12', \n",
    "            'frequencia_emitente_categoria_shift_12_rolling_12', \n",
    "            'flag_emitente_categoria_shift_12_rolling_12']\n",
    "\n",
    "\n",
    "# 3. TEXT COLUMN LIST (STR)\n",
    "str_vars = [\n",
    "    col for col in X_train.columns \n",
    "    if col not in num_vars and col not in blacklist\n",
    "]\n",
    "\n",
    "# 4. CODE COLUMN LIST (DIFFERENT TRANSFORMATION)\n",
    "cod_encode = ['cod_central','cod_agencia', 'cod_cooperativa']\n",
    "\n",
    "print(f'NumVars:\\n{X_train[num_vars].dtypes}')\n",
    "print(f'StrVars:\\n{X_train[str_vars].dtypes}')\n",
    "print(f'CodEncode:\\n{X_train[cod_encode].dtypes}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # -- PIPELINE --"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## --- PIPELINE TRANSFORMATION ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NUMBER PIPELINE TRANSFORMATION\n",
    "num_pipe = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# CATEGORICAL PIPELINE TRANSFORMATION\n",
    "str_pipe = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1))\n",
    "])\n",
    "\n",
    "# DATETIME PIPELINE TRANSFORMATION\n",
    "date_pipe = Pipeline([\n",
    "    ('eng', EngenhariaDatas()),\n",
    "    ('scaler', StandardScaler()) \n",
    "])\n",
    "\n",
    "# CODE PIPELINE TRANSFORMATION\n",
    "code_pipe = Pipeline([\n",
    "    ('encoder', TargetEncoder(target_type='continuous'))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## --- PREPROCESSOR (AUTOMATIZE NUMBER-STR-DATE-CODE TRANSFORMATION) ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('tr_num', num_pipe, num_vars),\n",
    "        ('tr_cat', str_pipe, str_vars),                 # CLASS IS ORDINAL, BUT ONEHOT WORKS FINE\n",
    "        ('tr_data', date_pipe, ['periodo']),            # COLUMN WILL BE TRANSFORM\n",
    "        ('tr_cods', code_pipe, cod_encode)\n",
    "        ],\n",
    "    remainder='drop',                                   # DROP ALL COLUMNS OUT NUM_PIPE OR STR_PIPE OR DATE_PIPE OR CODE_PIPE\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## --- FINAL PIPELINE (ENCAPSULATING ALL TRANSFORMATION IN PREPROCESSOR AND OTHER PIPELINES) ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pipe = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('modelo', Ridge(random_state=42, max_iter= 100))],\n",
    "    memory=None                                         # SAVE/NOT SAVE FIRST MODEL FIT TO OPTIMIZE PERFORMANCE IN MEMORY CACHE/SPEED TRAIN\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## --- FINAL PIPELINE PARAMS (TESTING DIFFERENT MODELS TO DISCOVER THE BEST) ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "params = [\n",
    "    # --- CENÃRIO 1: LIGHTGBM (O Veloz - Microsoft) - ATENÃ‡ÃƒO: Ã‰ muito leve e rÃ¡pido.  ---\n",
    "    {\n",
    "        'modelo': [LGBMRegressor(n_jobs=5, random_state=42, objective='tweedie')], # TWEEDIE - ENSURE MODEL UNDERSTAND DATA ASSIMATRY AND DECREASE PREDICT ERROR\n",
    "        'modelo__tweedie_variance_power': [1.4, 1.5],    # TWEEDIE DISTRIBUTION ADJUST VALUE (1.5 IS GOLD STANDARD 'Compound Poisson' (ZEROS + PEAKS)\n",
    "        'modelo__n_estimators': [1500, 2000],            # ESTIMATED NUMBER OF TREES \n",
    "        'modelo__learning_rate': [0.01, 0.05],           # STEP SIZE (ETA) | LOW = MOST ACCURATE, BUT TAKES MORE TIME\n",
    "        'modelo__num_leaves': [25, 31, 63],              # NUMBER OF NODES IN EACH BRANCH (MUCH HIGH VALUES = HIGH OVERFITTING PROBABILITY ) | MAIN PARAM FROM LIGHTGBM\n",
    "        'modelo__subsample': [0.8],                      # % RAMDOM SAMPLES OF ROWS FOR EACH TRAIN IN EACH BRANCH \n",
    "        'modelo__colsample_bytree': [0.7, 0.8],          # % RAMDOM SAMPLES OF COLS FOR EACH TRAIN IN EACH BRANCH |\n",
    "        'modelo__importance_type': ['gain']\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## --- SPLIT TIME SERIES AND CONFIG GRIDSEARCH ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# SPLIT TIME SERIES\n",
    "splitter_temp = TimeSeriesSplit(n_splits=3, gap=0)\n",
    "\n",
    "# CONFIGURATION GRIDSERACH PARAMS\n",
    "grid = GridSearchCV(\n",
    "    final_pipe,\n",
    "    param_grid = params,\n",
    "    cv= splitter_temp,\n",
    "    scoring='neg_mean_absolute_error', # 'neg_mean_absolute_percentage_error' FAVOR \"CONSERVATIVE\" MODELS : IF YOU RUN THIS GRIDSEARCH COMPARING W/ XGBOOST (COMMOM) AGAINST LIGHTGBM (TWEEDIE) USING MAPE AS A JUDGE: - XGBOOST WILL WIN AGAIN: XGBOOST (SCENE 2) PLAYS \"IN DEFENSIVE MODE\". IT TRIES LOW VALUES MAKE AN PERCENT LOW ERROR IN SMALL CONTRACTS. MAPE LOVES IT.\n",
    "    verbose= 4,\n",
    "    n_jobs= 1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # -- FITTING E PREDICTING MODEL GRIDSEARCH --"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## --- FITTING E MODEL GRIDSEARCH ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## --- PREDICTING W/ TEST DATABASE ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_pred = grid.best_estimator_.predict(X_test)\n",
    "\n",
    "# PREDICT RESULT VS REAL VALUES\n",
    "df_resultados = pd.DataFrame({\n",
    "    'Real': y_test,\n",
    "    'Previsto': y_pred\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # -- AUDITING MODEL --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# AUDITTING MODEL FUNCTION, VALIDATING FEATURE IMPORTANCE AND LIKELY DATA LEAKAGES AND PLOTTING ALL INFORMATION\n",
    "auditor = ModelAuditor(grid, X_test, y_test, groupcol=groupcol)\n",
    "auditor.run_full_audit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # -- TESTING GRIDSEARCH FITTED AGAINST OUT OF TIME BASE --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_oot = oot[target]\n",
    "X_oot = oot.drop(columns=[target], errors='ignore')\n",
    "y_oot_pred = grid.best_estimator_.predict(X_oot)\n",
    "\n",
    "fator_calibracao = FatorCalibracao(fator=fator_calibracao_final)\n",
    "y_oot_calibrado = fator_calibracao.transform(y_oot_pred)\n",
    "\n",
    "r2_oot = metrics.r2_score(y_oot , y_oot_pred)\n",
    "mse_oot = metrics.mean_squared_error(y_oot , y_oot_pred)\n",
    "rmse_oot = np.sqrt(mse_oot)\n",
    "soma_real_oot = y_oot.sum()\n",
    "soma_erro_abs_oot = np.abs(y_oot - y_oot_pred).sum()\n",
    "wmape_oot = soma_erro_abs_oot / soma_real_oot\n",
    "erro_medio = (y_oot_pred - y_oot).mean()\n",
    "viÃ©s_percentual = erro_medio / y_oot.mean()\n",
    "\n",
    "# RECALCULATE WMAPE\n",
    "novo_wmape = np.abs(y_oot - y_oot_calibrado).sum() / y_oot.sum()\n",
    "print(f'RÂ² Score: {r2_oot:.3f} vs\\n'\n",
    "        f'Mean Squared Error (MSE): {mse_oot:.3f} vs\\n'\n",
    "        f'Root Mean Squared Error (RMSE): {rmse_oot:.3f} vs\\n'\n",
    "        f'ðŸ’° WMAPE OOT (VisÃ£o Executiva): {wmape_oot:.2%} vs\\n'\n",
    "        f'ViÃ©s MÃ©dio (R$): {erro_medio:,.2f} vs\\n'\n",
    "        f'ViÃ©s Percentual: {viÃ©s_percentual:.2%} vs\\n'\n",
    "        f'ðŸ’° WMAPE Calibrado: {novo_wmape:.2%}')\n",
    "\n",
    "#PLOTING NEW PREDICT AGAINST OOT BASE\n",
    "df_oot = pd.DataFrame({\n",
    "    'Periodo': X_oot['periodo'],\n",
    "    'Real': y_oot,\n",
    "    'Previsto': y_oot_pred,\n",
    "    'Previsto_Ajustado': y_oot_calibrado\n",
    "    }\n",
    ")\n",
    "\n",
    "df_mensal = df_oot.groupby('Periodo')[['Real', 'Previsto','Previsto_Ajustado']].sum().reset_index()\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(df_mensal['Periodo'], df_mensal['Real'], label='Real', marker='o', linestyle='--')\n",
    "plt.plot(df_mensal['Periodo'], df_mensal['Previsto'], label='Previsto', marker='x', linestyle='--')\n",
    "plt.plot(df_mensal['Periodo'], df_mensal['Previsto_Ajustado'], label='Previsto_Ajustado', marker='x', linestyle='--')\n",
    "plt.title('Previsto Agregado por Periodo: OOT Real x Previsto')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.25)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
